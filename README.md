# LLM Agent记忆系统方法论综述

# 引言

大型语言模型（LLM）的出现标志着人工智能领域的一次范式转移，其强大的自然语言理解与生成能力为构建能够自主执行复杂任务的智能代理（Agent）奠定了基础。然而，LLM的核心架构——Transformer，存在一个固有的局限性：其上下文窗口（Context Window）是有限的。这一限制使得LLM本质上是无状态的（Stateless），在处理需要跨越多个交互轮次或长时间周期的任务时，会面临信息遗忘、无法维持对话连贯性以及难以从历史经验中学习等挑战 。

为了突破这一瓶颈，研究界提出了记忆增强型Agent（Memory-Augmented Agents）的理念。该范式不再将记忆仅仅视为一个外部数据库，而是将其视为Agent认知架构的核心组成部分，旨在赋予Agent持久化、自我演进和进行复杂状态推理的能力 。通过引入精心设计的记忆模块，Agent能够存储、检索、更新和利用历史信息，从而在长时程任务中保持状态一致性，并逐步积累知识与技能。

本综述报告将系统性地解构这些前沿工作，深入探讨记忆系统的核心技术轴线，包括多样化的储存架构（从简单的缓冲区到复杂的知识图谱）、对认知科学的借鉴（如情景记忆、语义记忆和程序化记忆）、将原始经验抽象为通用知识的关键过程，以及越来越多地采用基于学习的方法（尤其是强化学习）来实现动态记忆管理。



## 1\. 记忆储存的架构 (Memory Storage Architecture)

记忆储存架构是Agent记忆系统的基石，它定义了信息如何被组织、编码和关联。不同的架构选择直接影响了记忆系统的效率、可扩展性以及其能够支持的推理类型。本章将逐一剖析所选论文中提出的记忆储存架构，重点关注其使用的数据结构、记忆的编码方式以及记忆单元之间的链接机制。

### 1.1 论文分析

**1\. Contextual Experience Replay for Continual Learning of Language Agents (CER)**

- **架构方法**: CER采用一个"动态记忆缓冲区"（Dynamic Memory Buffer）来储存经验 。该架构并非一个严格定义的数据库或图结构，而是一个可变的、用于汇集提炼后经验的集合。
- **记忆编码方式**: 记忆被编码为自然语言描述。系统通过一个"蒸馏模块"将原始的交互轨迹处理成两种结构化的文本摘要：一是"环境动态"（Environment Dynamics），包含对网页的总结、对应的URL和推断出的潜在用途；二是"技能"（Skills），即具备通用性的多步决策模式，其中任务相关的具体实体被抽象为占位符 。编码的对象是这些经过高度抽象和概括的文本摘要，而非原始日志。
- **记忆间的链接**: 该框架没有描述显式的记忆链接机制。记忆单元之间的关联是隐式的和临时的，主要体现在两个方面：一是在在线学习模式下，经验按时间顺序被不断积累到缓冲区中；二是在检索阶段，系统根据当前任务的上下文来判断记忆的关联性，而非依赖预先建立的结构化链接 。

**2\. In-memory learning A declarative learning framework for large language models (IML)**
记忆以"笔记（notes）"形式存在于上下文中，完全以自然语言表示。系统将交互轨迹归纳为对环境规律的描述笔记，并在后续推理时更新这些笔记。无需向量数据库或图结构，记忆纯文本化、规则化存储，不同"笔记"之间也未显式关联。

- **架构方法**: IML的"记忆"架构极为独特，它直接利用LLM自身的上下文窗口作为唯一的记忆储存和操作空间。所有学习过程都通过在上下文中生成和修改自然语言"笔记"（Notes）来完成 。
- **记忆编码方式**: 记忆被编码为纯文本形式的"笔记"。这些笔记是从近期经验中提炼出的通用原则或规则，以陈述性语句的形式存在，用于指导模型未来的行为 。
- **记忆间的链接**: 记忆单元之间不存在结构化的链接。其关联性体现在一个演化过程中：新的笔记是基于近期经验生成的，并被用来"修正"（Revise）上下文中已存在的旧笔记。这种持续的迭代更新形成了一条概念上的知识演化链条，而非物理上的数据链接 。

**3\. Inducing Programmatic Skills for Agentic Tasks (ASI)**
记忆由两部分构成：**文本型记忆**和**程序库（skill library）**。文本型记忆可以视作传统的经验描述或提示；而程序库则存储自动诱导出的编程技能，每个技能以可执行的代码形式（如 Python 函数）存储。技能之间通过相似的输入输出结构或依赖关系隐式关联，但论文未给出明确图结构，更多依赖名称或功能匹配。记忆编码上主要是源代码文本，未使用向量嵌入。

- **架构方法**: ASI引入了一个"技能库"（Skill Library）作为其记忆储存架构 。这个技能库的核心特点是，它储存的不是陈述性知识，而是可执行的程序化技能。
- **记忆编码方式**: 记忆被编码为源代码，具体形式是Python函数。每一个函数都将一系列底层的原始动作（如click()、type()）封装成一个更高层次、可复用的技能(如search_product(name)) 。这种编码方式实现了从"是什么"的知识到"怎么做"的知识的转变。
- **记忆间的链接**: 链接是通过函数调用（Functional Composition）实现的。更高级的技能可以在其函数体内调用其他已经定义好的低级技能，从而自然地形成一个功能上的层次化调用结构。

**4\. Agent Workflow Memory (AWM)**
记忆以"工作流（workflow）"为单位组织，每个工作流包含一个文本描述和一系列按步骤描述任务的执行轨迹。工作流以自然语言和步骤列表形式存储，在提示上下文中提供给代理使用。没有使用向量数据库或复杂图结构，而是简单地把每个工作流作为一个文本条目（NL描述+步骤列表）加入记忆。工作流间通过共用的子步骤或抽象目标相互关联，但主要依赖后续检索筛选而非显式链接。

- **架构方法**: AWM构建了一个基于文本的"工作流记忆"（Workflow Memory），该记忆库用于扩充Agent已有的关于内置动作的基础记忆 。
- **记忆编码方式**: 记忆（即工作流）被编码为结构化的文本。每个工作流包含两部分：一个描述其高层目标的自然语言文本，以及一个完成该目标的步骤序列。序列中的每一步都详细记录了当时的环境状态描述、Agent的推理过程和最终执行的动作 。为了保证通用性，工作流中的实例特定信息（如具体的产品名称）会被抽象成变量（例如  
    {product-name}）。
- **记忆间的链接**: 链接机制是组合式的。更复杂的工作流可以通过将已有的、更简单的工作流作为其子任务来构建。这种组合关系是在工作流的"归纳"（Induction）阶段由LLM动态识别和建立的，而非在记忆库中通过显式的指针或边来维护 。

**5\. Memp: Exploring Agent Procedural Memory (Memp)**
记忆库（memory repository）存放两种层级的程序性记忆：细粒度的逐步指令和更抽象的脚本式流程。存储结构可视为列表或数据库，系统探索构建"键"（key）的方法以便检索，例如将记忆条目的内容或关键词编码为向量，或使用关键词匹配。没有提到知识图谱；主要是检索友好的条目存储（可能伴随向量键）。不同记忆之间可通过相似度或上下文共享关键词关联，但论文中未提到显式图结构链接。记忆条目编码为文本与可能的元数据（如关键词）组合。

- **架构方法**: Memp设计了一个程序化记忆的"仓库"（Repository），这是一个用于储存和管理从历史经验中提炼出的操作知识的集合 。
- **记忆编码方式**: 记忆被编码为两种不同粒度的文本：一是细粒度的、分步式的操作指令；二是更高层次的、类似脚本的抽象流程。这两种编码形式都是从Agent的过往交互轨迹中蒸馏而来 。
- **记忆间的链接**: 论文并未明确定义记忆单元间的链接方式。但其"仓库"概念，以及持续进行的内容更新、修正和废弃机制，暗示这是一个被主动管理的知识集合，其中的关联可能通过元数据或在检索时动态确定 。

**6\. AGENT KB: Leveraging Cross-Domain Experience for Agentic Problem Solving (AGENT KB)**
记忆构成一个多层次知识库，分为"工作流层面"和"执行层面"模式。其核心组件是一个时序知识图引擎 Graphiti，将对话和业务数据以图结构形式存储。数据节点代表实体或事件，边表示时序和因果关系。记忆编码既包含非结构化对话文本，也将结构化业务数据映射为图节点。不同记忆单元通过图的连边链接，体现时间序列和知识关系。

- **架构方法**: AGENT KB构建了一个共享的、层次化的知识库（Knowledge Base），这是一个为支持多Agent协作和跨领域知识迁移而设计的结构化记忆系统 。
- **记忆编码方式**: 经验被抽象并编码为结构化的元组（Tuple），其形式为 E=⟨π,γ,S,C,R⟩。其中，π 代表问题模式，γ 是可选的目标， S 是抽象后的解决方案轨迹， C 描述了问题特征，而 R 则代表了与其他经验的关联链接 。
- **记忆间的链接**: 该架构的核心特性之一就是显式的链接机制。元组中的 R 字段专门用于建立与其他经验单元的关系链接，使得系统能够将高层次的策略和底层的执行细节有机地联系起来，形成一个互联的知识网络 。

**7\. MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent (MemAgent)**
提出了一个固定长度的可读写记忆状态，与模型上下文并行存在。记忆被编码为与模型内部状态等效（可视为文本或隐向量的一部分）。工作流图中未描述，但记忆层可看作一个定长的上下文窗口片段或内部状态向量。该设计主要依赖强化学习控制何时何地"覆盖"先前记忆，无显式数据结构描述，重点在学习记忆更新策略。

- **架构方法**: MemAgent实现了一个固定长度的记忆系统，该系统完全在LLM的标准上下文窗口内运作。其记忆本身由普通的Token序列构成，从而确保了与现有Transformer架构的无缝兼容 。
- **记忆编码方式**: 记忆被编码为Token序列，它作为所有已处理信息的压缩表征。Agent通过学习来决定哪些Token应该被保留在这个固定大小的缓冲区中，从而实现对长文本的记忆 。
- **记忆间的链接**: 记忆单元的链接是时序性的，并体现为一种循环状态（Recurrent State）。在第 k 步的记忆 mk 是由第 k−1 步的记忆 mk−1 和新的上下文片段 ck 共同决定的，即 p(mk∣ck,mk−1)，这在Token层面建立了一条紧密的时序依赖链 。

**8\. Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework (Learn to Memorize)**
支持多种检索信号（如相关性、时序性等）的可学习组合。记忆内容源自代理在交互中获取的信息，将其存为检索友好的条目。具体数据结构不详，强调**混合检索（hybrid retrieval）**方式，而非仅静态存储。很可能采用了向量化编码并分段存储不同类型的记忆，但论文无详细说明图谱或键值架构。

- **架构方法**: 该工作提出了一个通用的、自适应的记忆框架，它并不预设某种具体的数据结构，而是将记忆的整个生命周期（检索、利用、储存）建模为一个由多个可优化组件构成的系统 。
- **记忆编码方式**: 论文的重点在于优化过程而非储存格式。但它提到，系统会从观察中提取关键信息进行储存，这暗示了记忆内容是经过处理的摘要或抽象文本 。
- **记忆间的链接**: 链接不是通过结构化数据实现的，而是在对记忆周期各组件的协同优化过程中被隐式地学习到。

**9\. Memento: Fine-tuning LLM Agents without Fine-tuning LLMs (Memento)**
将外部记忆视作一个**经验库（episodic memory）**，可为可微分或非参数化形式。记忆条目是之前交互（状态、动作、奖励）的集合，被存为案例库。通常用向量嵌入表示查询和记忆键值，虽然论文细节暂缺，但提及"记忆库和策略无缝结合"的思路，暗示使用向量近似检索。记忆条目之间无特殊连结，更类似于案例匹配系统。

- **架构方法**: Memento采用了一个名为"案例库"（Case Bank）的架构，用于储存作为情景记忆（Episodic Memory）的过往交互轨迹。这是一个在线增长的案例集合 。
- **记忆编码方式**: 每个记忆单元（案例）被编码为一个三元组 (s,a,r)，分别代表状态（state）、动作（action）和奖励（reward）。这种编码方式完整地记录了一次解决问题的尝试轨迹，保留了比简单摘要更丰富的上下文信息 。
- **记忆间的链接**: 在其非参数化版本中，案例之间是相互独立的，没有显式链接。而在其参数化变体中，系统会在线更新一个Q函数。这个Q函数通过为不同状态-动作对赋予价值，间接地塑造了案例的检索分布，从而在案例之间建立了一种基于价值的隐式关联 。

**10\. G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems (G-Memory)**
采用层级图结构管理多智能体记忆。其三层图包括"洞见（insight）图"、"查询（query）图"和"交互（interaction）图"。每一层代表知识的不同抽象层级：洞见图存储高层通用见解，交互图记录具体协作轨迹，查询图则连接查询与这些记忆。整体是一种图数据库架构，记忆节点为洞见或交互事件，边表示上下位关系和同源联系。

- **架构方法**: G-Memory专为多Agent系统（MAS）设计，其核心是一个三层图（Graph）层次结构。这三层分别是：**交互图**（Interaction Graph），记录细粒度的Agent间消息传递；**查询图**（Query Graph），将交互与特定的用户查询关联；以及**洞察图**（Insight Graph），储存从多个查询中抽象出的高层、可泛化的知识 。
- **记忆编码方式**: 记忆的主要编码形式是图结构本身。图中的节点代表Agent、消息、查询或洞察，其内容通常是文本。
- **记忆间的链接**: 这是一个完全基于图的链接架构。图的边（Edge）显式地定义了不同记忆单元之间的关系。例如，交互节点之间按时序和收发关系链接；查询节点链接到构成它的所有交互节点；洞察节点则链接到催生它的查询节点。这种设计构建了一个层次分明、关系丰富的记忆网络 。

**11\. Human-inspired Episodic Memory for Infinite Context LLMs (EM-LLM)**
将连续上下文划分为**事件**，并以事件为基本单元存储记忆。采用"贝叶斯惊奇+图细化"在线分段，将记忆组织成事件块。存储结构类似时间序列的事件列表（并用图算法处理边界）。每个事件可能附有向量嵌入，用于检索。记忆项间通过事件边界隐式相邻关联，也使用图优化事件划分，但不构建长期连边。

- **架构方法**: EM-LLM的架构直接作用于LLM的底层，它将模型的KV缓存（KV cache）组织成一系列连贯的"情景事件"（Episodic Events）。记忆库就是由这些被分割的KV缓存块组成的序列 。
- **记忆编码方式**: 记忆单元并非文本，而是模型内部状态的直接体现——即注意力机制中的键值对（Key-Value pairs）。这是一种非常底层的记忆编码方式。
- **记忆间的链接**: 链接主要是时序性的，事件按顺序排列。在分割阶段，系统会利用一个图论优化步骤，将注意力键（Key）之间的相似度视为一个加权邻接矩阵，以此来微调事件的边界。这在时序序列的局部建立了一种基于相似度的链接 。

**12\. From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs (MemTree)**
使用**动态树形**层次结构组织记忆。树的每个节点包含聚合的文本内容及其语义嵌入，节点层级对应抽象级别。新信息通过比较嵌入与已有节点适应树结构，实现上层概括和下层细节的分层存储。节点之间通过父子指针链接，形成记忆树。该架构以树数据库形式组织记忆。

- **架构方法**: MemTree采用动态树（Dynamic Tree）结构。树中的每个节点都包含了聚合后的文本内容、该文本的语义嵌入向量，以及指向其父节点和子节点的指针 。
- **记忆编码方式**: 每个节点同时储存两种编码形式：人类可读的聚合文本和机器易于处理的向量嵌入。树的浅层节点代表更抽象的概念，而深层节点则包含更具体的信息，形成了一个从概括到细节的层次结构 。
- **记忆间的链接**: 架构本身就是一种显式的层次化链接。新的信息通过遍历树结构，根据语义相似度与现有节点进行比较，最终决定是与某个节点合并还是作为新的叶子节点插入，从而动态地扩展这个链接网络 。

**13\. Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning (Memory-R1)**
引入了双代理的**外部记忆库**。记忆存储为可增删改的条目（如问答对），由"记忆管理器"学习操作（增删改）。具体数据结构未详述，推测为键值对列表。没有提到图或结构化存储，更多关注如何通过RL决定记忆写入与检索行为。

- **架构方法**: Memory-R1使用一个外部的"记忆库"（Memory Bank）。论文没有详细说明其底层数据结构，但其基于RAG的检索方式表明，它很可能是一个向量数据库或类似的、支持高效索引的集合。
- **记忆编码方式**: 记忆以文本事实或陈述的形式储存（例如，"安德鲁收养了两只狗，分别叫巴迪和斯考特"）。这些文本在存入时很可能会被转换为向量嵌入，以支持后续的相似度检索。
- **记忆间的链接**: 论文没有描述显式的链接机制。记忆库中的每个记忆单元被视为独立的事实，它们之间的关联性是在检索时通过与用户查询的相似度来动态确定的。

**14\. Coarse-to-Fine Grounded Memory for LLM Agent Planning (CFGM)**
构造两级记忆：粗粒度焦点和细粒度提示。粗粒度层存储场景的关键点（如主要环境特征），细粒度层存储可行动的提示或经验。

- **架构方法**: CFGM是一个多阶段的记忆系统，主要由一个"经验池"（Experience Pool）和一个"技巧字典"（Tips Dictionary）构成 。
- **记忆编码方式**: 记忆以不同粒度的形式存在：粗粒度的"焦点"（Focus Points），是指导性的文本；完整的交互轨迹，包括成功和失败的尝试；以及混合粒度的"技巧"（Tips），是从对比成功与失败轨迹中提取出的可操作建议文本 。
- **记忆间的链接**: 链接是基于上下文的。技巧与催生它们的任务相关联；经验池中的轨迹则与指导其收集过程的焦点相关联。

**15\. Multiple Memory Systems for Enhancing the Long-term Memory of Agent (MMS)**
受到认知心理学启发，将短期记忆加工成多个长期记忆片段。它定义了"检索记忆单元"和"上下文记忆单元"成对存储，一一对应。数据结构类似：记忆由一系列单元对组成，每对含检索键和丰富上下文信息。检索单元用于匹配查询，对应单元用于提供应答时的上下文。这种双组件对没有使用图结构，而是形成一对一的检索映射系统。

- **架构方法**: MMS采用一种双单元（Dual-unit）系统架构。对于每一条信息，系统都会创建两个相互对应的记忆单元："检索记忆单元"（Retrieval Memory Unit）和"上下文记忆单元"（Contextual Memory Unit）。
- **记忆编码方式**: 受认知心理学启发，系统会对短期记忆进行深度处理，提取出多种"记忆片段"，包括：关键词、多维认知视角、情景记忆（与特定事件相关）和语义记忆（事实性知识）。检索单元由部分易于匹配的片段构成，而上下文单元则包含更丰富、完整的信息 。
- **记忆间的链接**: 该架构的核心特点是，每个检索单元都与其对应的上下文单元之间存在一条直接的、一对一的链接 。

**16\. Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory (TiM)**
提出将"思考结果（thoughts）"存入记忆，使模型免于重复推理。记忆单元即保存的"思考"。为高效检索，引入局部敏感哈希（LSH），暗示记忆条目已被嵌入为向量以便哈希索引。每个记忆项是一个文本"思考"，伴有向量索引。记忆项之间没有显式链接，主要依赖LSH搜索实现相关性检索。

- **架构方法**: TiM使用一个"记忆缓存"（Memory Cache）来储存"想法"（Thoughts）。为了高效管理这个缓存，系统采用了局部敏感哈希（Locality-Sensitive Hashing, LSH）技术进行索引 。
- **记忆编码方式**: 储存的单元不是原始的对话历史，而是经过LLM在交互后"后思考"（Post-thinking）阶段生成的"想法"。这些想法是提炼过的、归纳性的结论，以文本形式保存，代表了推理路径或关键事实的精华 。
- **记忆间的链接**: 系统没有显式的图状链接。但是，它支持"合并"（Merge）操作，可以将关于同一实体的相似想法融合成一个更全面、综合的记忆单元，从而在内容层面建立了隐式的关联 。

**17\. A-MEM: Agentic Memory for LLM Agents (A-MEM)**
受到 Zettelkasten 方法启发，设计了**动态图知识网络**。每次添加新记忆时，会生成包含上下文描述、关键词、标签等属性的结构化笔记。系统扫描历史记忆，自动识别和创建相似性链接，将相关笔记连接成网络。这实现了记忆节点（笔记）和它们之间的有向边（主题或关键词相似性）的图结构。记忆编码为结构化文本笔记，可看作图数据库。

- **架构方法**: A-MEM的设计灵感来源于"卡片盒笔记法"（Zettelkasten method），旨在创建一个互联的知识网络。它使用向量数据库（ChromaDB）作为底层存储，以支持相似度搜索 。
- **记忆编码方式**: 每个记忆单元都是一张"综合笔记"（Comprehensive Note），包含多个结构化属性：原始交互内容、由LLM生成的上下文描述、关键词和标签。同时，整张笔记也会被编码成一个向量嵌入 。
- **记忆间的链接**: 动态链接是其核心功能。当一张新笔记被创建时，系统会主动分析历史记忆库，通过识别共享属性或语义相似性，在新笔记与旧笔记之间建立显式的链接，从而逐步构建出一个图状的知识网络 。

**18\. Zep: A Temporal Knowledge Graph Architecture for Agent Memory (Zep)**
其核心组件 **Graphiti** 构建了一个**时序知识图**。此知识图将对话和业务数据映射为节点，节点之间按时间和语义连接。记忆通过向图中添加带有时间戳的事实（对话片段、业务实体等）来存储，从而支持跨会话和跨源信息的集成。是典型的知识图谱结构。

- **架构方法**: Zep的核心是一个名为"Graphiti"的时态知识图谱（Temporal Knowledge Graph）。该图谱由三个相互关联的子图构成：**情景子图**（Episode Subgraph），储存原始输入数据；**语义实体子图**（Semantic Entity Subgraph），包含提取出的实体及其关系；以及**社区子图**（Community Subgraph），代表了紧密关联的实体簇 。
- **记忆编码方式**: 数据被结构化地编码为图中的节点（实体、情景、社区）和边（关系）。节点和边都附有属性，其中最关键的是时态信息。系统采用双时态模型（Bi-temporal Model），同时追踪事件的发生时间（Chronological Time）和数据入库的系统时间（Transactional Time）。
- **记忆间的链接**: 这是一个完全由显式链接定义的架构。图中的边明确地标识了实体与情景之间的关系。双时态模型的引入为这些链接增加了一个至关重要的维度，使得系统能够推理关系是如何随时间演变的 。

**19\. MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents (MEM1)**
该方法并未采用显式外部记忆数据库，而是通过维护一个紧凑的**内部状态向量**来融合记忆与观察。即每个对话轮次后更新一个固定大小的内部隐藏状态（内存），该状态兼具记忆和推理功能。数据结构上类似固定大小的"工作记忆"向量，没有显式条目存储，也无图结构。

- **架构方法**: MEM1的记忆是一个紧凑、共享的"内部状态"（Internal State），它在Agent的上下文窗口内被持续更新。系统使用结构化的XML风格标签（如&lt;IS&gt;）来界定这个记忆块 。
- **记忆编码方式**: 内部状态被编码为文本，是所有先前交互和Agent推理过程的综合摘要。它是一个不断演进的、对任务历史的高度压缩的表征 。
- **记忆间的链接**: 链接是纯粹时序性和循环性的。在 t+1 轮次的内部状态是 t 轮次状态和新观察信息的直接函数。旧的状态会从上下文中被修剪掉，使得新的状态成为连接过去的唯一纽带 。

**20\. Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science (Nemori)**
提出"自组织"记忆架构，首先根据事件分割理论将对话流按语义聚类为**事件片段**。每个事件（episode）作为记忆单位存储，类似时间序列列表。然后基于预测-校正原则动态更新知识。记忆条目即时间顺序的事件摘要，没有明确图结构连接，事件间靠时间顺序和语义相似性自然衔接。

- **架构方法**: Nemori是一个自组织的记忆架构，它将连续的对话流分割成语义上连贯的"情景"（Episodes）。
- **记忆编码方式**: 记忆以文本情景的形式储存。此外，系统还会从这些情景中蒸馏出关键知识，形成"语义记忆"（Semantic Memory），从而构成一种双重表征系统 。
- **记忆间的链接**: 论文没有定义显式的图结构链接。但其分割和蒸馏的过程暗示了一种层次关系：语义记忆来源于情景，因此在概念上与它们所源自的情景相关联。

**21\. Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory (Mem0)**
引入了可扩展长时记忆机制。基本版本动态提取对话中的关键信息并存储，增强版本进一步使用**图结构**捕获会话元素间的关系。具体而言，Mem0 的图形记忆表示将对话事实作为节点，把元素间的关联（如主题相似、实体共现）作为边。其余版本则更类似扁平的事实列表。总的来说，Mem0 有平面提取版本和图版，皆以持久对话事实的形式存储记忆。

- **架构方法**: Mem0提供了两种架构变体。基础版Mem0使用标准的键值对或向量存储。增强版Mem0g则采用有向标记图（Directed Labeled Graph）。
- **记忆编码方式**: 在Mem0中，记忆单元是从对话中提取的简洁文本事实。而在Mem0g中，记忆被编码为图结构，其中节点代表实体（如人物、地点），边则代表它们之间的关系 。
- **记忆间的链接**: 在Mem0g版本中，链接是通过图的边来显式定义的，这使得系统能够捕捉和利用对话元素之间复杂的关系结构 。

### 1.2 本章总结

对这21篇论文的分析揭示了LLM Agent记忆储存架构设计的几个核心趋势和演化路径。

首先，**架构的复杂性与任务需求紧密相关，呈现出一个从简单到高度结构化的谱系。** 对于特定任务，如网页导航，简单的"动态缓冲区"（CER）或程序代码库（ASI）已足够高效。而当任务转向需要长期连贯对话或多Agent协作时，架构的复杂性显著增加，催生了如层次化知识库（AGENT KB）、动态树（MemTree）乃至多层时态知识图谱（G-Memory, Zep）等更为精密的结构。这表明，不存在一种"万能"的记忆架构，最优的设计选择是由Agent所需执行的任务类型和推理复杂度决定的。

其次，**借鉴认知科学理论和人类知识管理方法论已成为一种重要的设计思潮。** 许多先进的系统不再将记忆视为简单的信息仓库，而是试图构建一个模拟人类认知过程的架构。例如，MMS和Zep明确区分了情景记忆（对具体事件的回忆）和语义记忆（对事实和概念的理解）；A-MEM则将"卡片盒笔记法"这一成熟的个人知识管理体系引入Agent设计中 ；而MemTree则致力于模拟人类认知中的层次化图式（Hierarchical Schemas）。这一趋势反映出研究者们的一个共识：要实现类人的长期推理和知识整合能力，模仿人类大脑和认知系统已被验证有效的组织结构，是一条极具前景的技术路径。这标志着记忆系统的设计理念正从"作为存储的记忆"向"作为认知架构的记忆"转变。

最后，**双重编码策略成为兼顾效率与质量的实用选择。** 在多个先进的系统中（如MemTree, A-MEM, Zep），记忆单元通常被同时编码为两种形式：机器友好的向量嵌入和人类可读的文本。向量嵌入对于执行高效的、大规模的语义相似度检索至关重要，是快速定位相关记忆候选项的基础 。然而，嵌入是一种有损的、不透明的表示。保留原始或聚合后的文本，对于LLM进行精细的逻辑推理、生成最终的、有理有据的响应，以及保证系统的可解释性，都是不可或缺的。这种混合编码方法巧妙地平衡了检索效率和推理质量之间的矛盾，已成为构建高性能记忆系统的一种事实标准。

下表总结了每篇论文在记忆储存架构方面的核心方法。

| **论文** | **核心架构与方法概述** |
| --- | --- |
| CER(Contextual Experience Replay) | 使用动态经验缓冲区，储存由环境动力学和技能组成的经历，每个经历转化为文本描述存入记忆；采用非结构化文本存储，无向量索引。 |
| In-Memory Learning | 将经验归纳为"笔记"，在上下文中以自然语言形式存储并迭代更新；记忆为文本规则集合，无专门的数据结构或向量存储。 |
| ASI(Inducing Programmatic Skills) | 记忆包括文本提示和程序库；编程技能以可执行代码形式存储（Python 函数）；无图结构，主要以代码和对应元数据（名称、说明）管理记忆。 |
| AWM | 以"工作流"条目形式存储记忆，每个工作流含文本描述和步骤列表；作为普通文本注入上下文；无向量数据库或图链接，工作流间通过共现步骤隐式关联。 |
| Memp | 记忆库包含**细粒度步骤指令**和**脚本式抽象**；存储为文本条目，可采用关键词或向量键构建索引；带有动态更新机制（增删改）。 |
| AGENT KB | 分层知识库（工作流层+执行层），核心为时间感知知识图（Graphiti）；以图数据库形式保存对话与业务数据；节点表示策略/事件，边表示时间和语义关系。 |
| MemAgent | 维护固定长度内部上下文记忆，不显式存储单条记忆，主要靠RL策略在线更新记忆窗口；没有外部结构化存储。 |
| Learn to Memorize | 构造混合检索记忆系统，可能以多种嵌入方式存储记忆条目；强调多信号检索（相关性、时序等），无明确说明结构但暗示使用向量化记忆表。 |
| Memento | 外部经验库（episodic memory），记忆条目为先前交互案例, 以(状态, 动作, 奖励)元组形式储存完整的情景记忆轨迹。；可微分或非参数化形式；内部通过神经策略检索适合案例，无图结构。 |
| G-Memory | 三层级图结构（洞见图、查询图、交互图）管理记忆；在知识图中存储高层见解和细粒度协作轨迹；节点与边体现跨任务和跨智能体的知识共享。 |
| EM-LLM | 将对话分割为**事件块**并存储，采用在线事件分割算法（贝叶斯惊奇+图细化）；记忆条目为事件列表，可为每个事件计算嵌入；检索结合相似度和时间邻近。 |
| MemTree | 动态**树形结构**存储记忆，节点按层次聚合文本内容并附带语义嵌入；新信息按嵌入相似度在树上插入；节点通过树边相连，支持多级抽象。 |
| Memory-R1 | 外部记忆库以条目列表形式维护（如问答对）；"记忆管理器"学习增删改查操作；重点不在数据结构，推测使用键值索引或向量检索，无图数据库。 |
| Coarse-to-Fine | 双层记忆：粗粒度焦点（关键环境信息）与细粒度提示（具体可执行信息），均以文本存储；无明确向量化或图结构描述，依赖提示内容的语义重用。 |
| Multiple Memory (MMS) | 设计多重记忆单元对，每对包含"检索单元"和对应"上下文单元"；检索单元用于匹配查询，上下文单元用于提供信息；整体类似键值对存储，无图结构。 |
| TiM | 将对话中的"思考"结果作为记忆项存储；使用LSH对记忆向量进行索引，实现高效检索；记忆单元为文本思考串及其向量签名，无显式连接。 |
| A-MEM | 模仿 Zettelkasten，生成带有描述、关键词、标签等属性的结构化笔记；通过分析新旧记忆找出相似笔记并连成网络（知识图）；既存记忆随新记忆动态更新属性，整体为图结构。 |
| Zep | 基于**时序知识图**构建记忆层（Graphiti）；对话和企业数据作为节点加入图中，节点间按历史关系连接；支持动态更新，强调跨会话的长期信息集成。 |
| MEM1 | 维护一个**共享内部状态**（固定尺寸向量）集成记忆与当前观察；相当于紧凑的"工作记忆"，定期更新并丢弃无关信息；无外部数据存储结构。 |
| Nemori | 采用自上而下事件分割把对话组织成连贯"事件"（episode）；记忆单位即事件文本；并通过"预测-校正"原则动态演化知识；记忆按事件顺序无明确图关联。 |
| Mem0 | Mem0 基础版提取对话关键信息存文本记忆，增强版使用**图形存储**（将对话实体与关系构成图）；存储事实节点及其关联；图版本捕获复杂关系，普通版本则扁平列表。 |

#### 按架构方法归纳的论文分类

| **架构方法** | **代表论文** | **核心特点** |
| --- | --- | --- |
| **缓冲区/仓库型** | CER, AWM, Memp, Coarse-to-Fine, MMS | 使用集合或列表存储记忆条目，注重动态更新和管理，适合特定任务场景 |
| **图结构型** | AGENT KB, G-Memory, MemTree, A-MEM, Zep, Mem0g | 基于图数据库或知识图谱组织记忆，支持复杂关系和层次化表示 |
| **树结构型** | MemTree | 采用层次化树结构，支持从概括到细节的多级抽象 |
| **内部状态型** | MemAgent, MEM1 | 将记忆融入模型内部上下文或状态向量，无外部存储 |
| **上下文窗口型** | In-Memory Learning, EM-LLM | 直接利用LLM上下文作为记忆空间，通过分割或笔记管理 |
| **程序化/技能型** | ASI | 存储可执行代码和技能，支持函数式组合 |
| **混合检索型** | Learn to Memorize, Memento, Memory-R1, TiM | 结合多种检索信号和学习策略，注重自适应管理 |

#### 按编码方式归纳的论文分类

| **编码方式** | **代表论文** | **核心特点** |
| --- | --- | --- |
| **纯文本型** | CER, In-Memory Learning, AWM, Coarse-to-Fine | 记忆以自然语言文本形式存储，便于人类理解和直接使用 |
| **结构化文本型** | ASI, Memp, MemTree, A-MEM | 包含元数据、关键词、标签等结构化属性的文本编码 |
| **源代码型** | ASI | 记忆编码为可执行的编程代码，支持函数式操作 |
| **向量嵌入型** | Learn to Memorize, TiM, Mem0 | 结合文本和向量表示，支持高效相似度计算 |
| **双重编码型** | MemTree, A-MEM, Zep | 同时使用文本和向量嵌入，平衡可读性和检索效率 |
| **底层状态型** | MemAgent, EM-LLM | 基于模型内部状态（如Token序列、KV缓存）编码 |
| **元组/结构化数据型** | AGENT KB, Memento, MMS | 使用结构化数据格式存储记忆，支持复杂关系建模 |

#### 按记忆间链接方式归纳的论文分类

| **链接方式** | **代表论文** | **核心特点** |
| --- | --- | --- |
| **无显式链接/隐式关联** | CER, In-Memory Learning, AWM, Coarse-to-Fine, MMS | 记忆单元相对独立，通过检索时动态确定关联 |
| **时序链接** | MemAgent, EM-LLM, MEM1, Nemori | 基于时间顺序或循环状态建立记忆间的连续性 |
| **图状链接** | AGENT KB, G-Memory, MemTree, A-MEM, Zep, Mem0g | 通过图的边显式定义记忆单元间的关系结构 |
| **层次化链接** | MemTree, G-Memory | 支持多层次抽象和上下位关系的链接 |
| **函数式链接** | ASI | 通过函数调用和组合建立技能间的依赖关系 |
| **相似度链接** | TiM, A-MEM | 基于语义相似度动态建立链接，支持内容合并 |
| **无链接** | Learn to Memorize, Memento, Memory-R1 | 结合多种链接机制，通过学习优化关联方式 |

## 2\. 记忆使用与检索方法 (Memory Usage and Retrieval)

如果说储存架构是记忆的静态骨架，那么使用与检索方法则是赋予其生命力的动态血液循环系统。它决定了Agent如何在恰当的时机，从海量的历史信息中高效、准确地找到所需内容，并以有效的方式加以利用。本章将深入分析各论文提出的检索机制、训练方法、后处理步骤以及在多Agent场景下的特殊设计。

### 2.1 论文分析

**1\. Contextual Experience Replay for Continual Learning of Language Agents (CER)**
检索通过提示语完成，而非传统向量检索。具体方法是将当前任务信息（目标、页面描述等）与缓冲区中所有经历一起提示给模型（如GPT-4），然后模型输出最相关的 top-k 经验作为检索结果，无须额外训练。检索后直接将这些经验插入上下文中辅助决策，没有额外的二次排序；如果发现新经验与已有经历重复则合并。。该系统无多智能体记忆分发，只服务单一代理。使用时不断执行经验蒸馏、合并更新来维护内存，旧经验若与新发现冲突，可选择覆盖更新。

- **检索机制**: 检索是一个免训练（Training-free）的过程，完全依赖于对一个大型视觉语言模型（VLM）的提示（Prompting）。系统将当前的任务目标、网站描述以及记忆缓冲区中所有可用的"动态"和"技能"作为上下文提供给VLM，并指令其选出Top-k个最有用、信息量最丰富的经验。VLM通过返回这些经验的ID来完成检索 。
- **检索模块的训练**: 检索模块未经专门训练。
- **检索后处理**: 检索到的k个经验会通过一个程序化的映射函数，被转换成格式化的自然语言描述。这些描述随后被直接注入到Agent当前决策的上下文窗口中，从而增强其决策能力 。
- **多Agent场景下的记忆分发**: 未涉及。

**2\. In-memory learning A declarative learning framework for large language models (IML)**
检索即使用内存笔记直接参与推理。代理在推理阶段将当前任务输入与现有笔记一起输入模型，没有额外检索算法；没有向量检索或关键词匹配。笔记更新在"归纳-修正"阶段完成。无多智能体问题，无记忆广播。代理每次使用笔记回答后进入"后思考"阶段，将结果整合回笔记，即记忆动态更新。

- **检索机制**: 由于所有"记忆"（即笔记）都存在于上下文窗口中，因此不存在传统意义上的"检索"。LLM在生成下一步动作时，会自然地利用其注意力机制（Attention Mechanism）来"关注"上下文中所有相关的笔记。
- **检索模块的训练**: 不适用。
- **检索后处理**: 不适用。
- **使用时的额外操作**: 核心的额外操作是"修正"（Revision）。在"归纳"（Induction）阶段生成新笔记后，Agent会利用这些新笔记来更新或重写已有的旧笔记，从而实现知识的迭代和演进 。

**3\. Inducing Programmatic Skills for Agentic Tasks (ASI)**
检索机制体现在代理调用程序技能的过程，代理通过语言模型决定是否使用某项编程技能。没有向量索引；模型可直接生成或调用某项技能函数（作为工具）。由于技能的引入为非参数化，故不需要专门训练检索模块。检索相当于"选择可用函数"这一动作，无复杂后处理。多智能体情景下，作者提到技能可跨网站复用，但仍在单代理上下文中使用。

- **检索机制**: 论文未详细描述一个复杂的检索机制。技能被添加到Agent的可用"动作空间"（Action Space）中。当Agent决策时，LLM会像选择基础动作（如click）一样，从这个扩展后的动作空间中选择调用某个已学习的技能（程序函数）。
- **检索模块的训练**: 不适用。
- **检索后处理**: 在技能被添加到技能库之前，会经过一个严格的"验证"（Verification）步骤。系统会执行新技能，并检查任务是否能被成功解决，以此确保加入记忆的技能是正确且有效的 。
- **使用时的额外操作**: 技能在使用时是作为可执行代码被调用的，这与基于文本的记忆有本质区别。

**4\. Agent Workflow Memory (AWM)**
检索过程即从已诱导工作流中选取相关条目。具体方法未详，但论文强调"选择性提供工作流"来辅助当前任务，推测通过对当前目标与工作流描述进行匹配（如模型或规则筛选相关工作流）。没有训练检索器，可能使用提示或简单匹配。检索后无需重排序，只将相关工作流以文本形式附加入上下文。无多智能体设置。额外操作包括：从正确解决过的任务中离线或在线抽取新工作流并加入记忆；无时序动态修正机制，仅靠不断引入新工作流实现适应。

- **检索机制**: 类似于ASI，AWM也没有描述一个独立的检索模块。诱导出的工作流被直接整合进Agent的记忆中，作为其上下文的一部分。LLM在决策时，会根据当前任务和观察，在其推理过程中自然地选择遵循某个工作流的指导 。
- **检索模块的训练**: 不适用。
- **检索后处理**: 未提及。

**5\. Memp: Exploring Agent Procedural Memory (Memp)**
提供了几种检索策略。首先根据环境上下文构造查询向量，然后使用"键构建策略"进行向量检索：可以是基于查询本身嵌入、或从记忆条目中提取关键词再进行匹配。检索器未明确训练，但作者对比了多种检索方案（如 query-vector vs keyword-vector）。检索后对记忆动态更新：包含新增条目、过滤不相关部分、合并或废弃过时片段。没有多Agent记忆分发。使用过程中会持续"反省"并修改记忆——例如发现错误或冗余时删除条目，这是其记忆更新的特点。

- **检索机制**: 论文探讨了多种不同的检索策略，包括基于查询向量匹配和基于关键词-向量混合匹配等方法，旨在研究如何更精确地构建和检索程序化记忆 。
- **检索模块的训练**: 未明确提及训练，但对不同策略的探索暗示了其可配置性。
- **检索后处理**: 未提及。
- **使用时的额外操作**: Memp的一个核心特点是引入了多样的记忆"更新"（Update）策略，包括修正、合并和废弃过时内容，使得记忆库能够与Agent的经验同步演化 。

**6\. AGENT KB: Leveraging Cross-Domain Experience for Agentic Problem Solving (AGENT KB)**
采用"双阶段检索"机制。学生代理首先根据查询在"工作流级别"检索指导策略（可能类似查询工作流知识库），教师代理则进一步检索具体的执行日志来完善答案。检索不依赖余弦匹配，而是通过图查询（推理-检索-精炼流水线）方式获取信息。检索模块本身未特别训练，使用现有框架的查询机制。检索结果会经过"精炼"（refine）阶段，即模型使用检索到的高层洞见和底层轨迹共同推理。多智能体场景下记忆是共享的知识库，所有代理可访问同一图；无角色差异分发。额外操作包括查询后对策略微调：教师代理会基于检索结果帮助代理完成任务，可能引入记忆更新（吸收新轨迹到知识图）。

- **检索机制**: 采用一个"推理-检索-精炼"（Reason-Retrieve-Refine）的流水线。Agent首先对新查询进行"推理"以理解其核心问题，然后从知识库中"检索"相关的、工作流级别的经验 。
- **检索模块的训练**: 未提及。
- **检索后处理**: 一个关键的后处理步骤是"精炼"（Refine）。Agent需要将检索到的通用工作流适配到当前任务的具体上下文中。这个过程可能涉及实体映射（例如，将一个网站的用户名字段映射到另一个网站）、工具替换等适应性调整 。
- **多Agent场景下的记忆分发**: AGENT KB被设计为一个共享的知识库，允许多个Agent从中学习和向其贡献，实现了跨Agent的知识传递 。

**7\. MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent (MemAgent)**
检索其实是**状态更新**：在每个对话轮次，代理决定是否将当前对话"读入记忆"，并更新固定长度记忆状态。作者采用RL (DAPO算法)训练这一存储策略；每轮记忆更新动作（overwrite）被视为策略网络的输出。没有传统的检索模块，只是将新的上下文与旧记忆融合，低频"刷新"掉无关信息。检索后处理主要体现在"何时清除旧记忆"的动作。没有多Agent记忆分发。

- **检索机制**: 不存在传统检索。记忆是Agent的持续内部状态。
- **检索模块的训练**: 记忆的更新过程是其核心，这个过程被建模为一个强化学习（RL）问题并进行训练。系统采用了一种名为Multi-Conv DAPO的新型RL算法，该算法专门设计用于处理多个独立的、但共同导向一个最终结果的对话上下文。奖励信号基于最终答案的正确性，通过RL训练，模型学会了何种信息应该被保留在固定长度的记忆中，何种信息可以被丢弃 。
- **检索后处理**: 不适用。

**8\. Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework (Learn to Memorize)**
检索采用混合专家门控机制。具体地，将当前查询与记忆中的多种"记忆角度"（如最新消息、有价值历史等）计算相关性，然后由可学习的门控网络（MoE）加权合并。检索模块通过一定的强化/对比学习训练来优化，这里论文提到使用经验回放和策略梯度来提升检索质量。初筛后，会对选出的小量记忆条目进行聚合处理：论文提出可学习的聚合函数，将多条检索结果融合为一段输入给模型。没有多Agent方案，仅单用户聊天场景。使用时还考虑到冗余过滤及时序调整——例如对已使用的记忆降低权重，或根据当前对话进度动态更新检索偏好。

- **检索机制**: 检索过程是可学习的。系统采用了一个混合专家（Mixture-of-Experts, MoE）门控函数，该函数可以根据当前上下文动态地调整多个检索指标（如语义相似度、时间近因性、重要性等）的权重。这超越了传统方法中手动设定或固定权重的组合方式 。
- **检索模块的训练**: 检索模块（即MoE门控网络）可以通过对比学习等方法进行离线优化，或在与环境交互中进行在线（On-policy）优化 。
- **检索后处理**: 论文提出了一种可学习的聚合过程来更好地利用检索到的记忆，而不仅仅是简单地将它们拼接起来。这个过程可以通过直接偏好优化（DPO）等技术进行对齐 。

**9\. Memento: Fine-tuning LLM Agents without Fine-tuning LLMs (Memento)**
该方法显式训练了**记忆读写策略**。Memory-Manager 和 Answer-Agent 分别通过强化学习学习何时写入、更新或删除记忆，以及如何检索相关记忆来回答问题。检索上，Answer-Agent 通过模型推理选择最相关的记忆条目（可能基于匹配分数），再结合条目生成答案。检索模块（Answer-Agent）通过PPO等RL算法微调。对检索结果没有额外二次排序，直接由模型融合回答。无多智能体内容。使用时的附加操作包括**记忆重写机制**：模型可以修改存储条目内容（update）或丢弃过时条目，以保持记忆库高效。

- **检索机制**: 检索基于案例推理（Case-Based Reasoning），核心是相似度匹配。在参数化版本中，检索过程由一个在线学习的Q函数引导，这个Q函数形成了一个案例选择策略，使得检索本身具有自适应性 。
- **检索模块的训练**: Q函数（即检索策略）通过在线强化学习持续更新。环境反馈（奖励）被用来更新Q值，从而改进未来的案例选择 。
- **检索后处理**: 未提及。
- **使用时的额外操作**: 记忆库是在线增长的，并且存在一个"记忆重写"（Memory Rewriting）机制，根据环境反馈来更新策略 。

**10\. G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems (G-Memory)**
检索通过双向遍历图来实现。当收到新查询时，系统同时沿"查询图"向上搜索获取高层洞见，并沿"交互图"向下搜索相关的具体协作轨迹。这是基于图结构的检索算法，无训练过程。初筛出相关子图节点后，模型综合高层见解和底层轨迹进行任务执行。由于是多代理系统，所有智能体共享同一层级记忆图，每个查询从全局内存提取信息。没有简单关键词过滤，而是依托图中连接；无需二次排序，因为图自带优先级（如洞见图一般广度优先）。额外操作：任务完成后将新交互加入图中不断扩充记忆层级，使记忆"自我进化"。

- **检索机制**: 采用一种双向记忆遍历（Bi-directional Memory Traversal）策略。当收到新查询时，系统会同时进行自顶向下和自底向上的检索：从"洞察图"中检索高层次、可泛化的洞察，同时从"交互图"中检索细粒度的、与当前任务直接相关的交互轨迹。这种机制为Agent提供了多粒度的上下文信息 。
- **检索模块的训练**: 未提及。
- **检索后处理**: 未提及。
- **多Agent场景下的记忆分发**: G-Memory是为多Agent系统设计的全局共享记忆。检索到的记忆会根据需要分发给系统中的不同Agent，以支持它们的协同工作。

**11\. Human-inspired Episodic Memory for Infinite Context LLMs (EM-LLM)**
采用"两阶段检索"。第一阶段基于**相似度检索**：将当前对话片段的嵌入与已存事件嵌入做最近邻匹配。第二阶段基于**时间邻近**：检索当时段前后的临近事件来补充上下文。两个阶段输出的记忆事件结合起来作为查询结果，无额外训练过程。检索后处理方面，无复杂二次排序，只简单地合并这两类事件片段。无多Agent设置。额外操作：基于事件划分动态管理记忆单元，过时事件根据与新查询的相似度或时间远近决定是否遗忘。

- **检索机制**: 采用一个两阶段的检索过程。第一阶段是基于相似度的检索，快速定位可能相关的"情景事件"；第二阶段则结合了时间连续性（Temporally Contiguous）检索，即也会检索与第一阶段结果在时间上相邻的事件。这种设计模仿了人类记忆的联想和时序特性 。
- **检索模块的训练**: 免训练。
- **检索后处理**: 未提及。

**12\. From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs (MemTree)**
检索时遍历树结构：从根节点向下对比语义嵌入或计算相似度，动态选择匹配的新信息应插入或合并到哪个子节点。没有显式训练阶段，算法自动调整树。更新后处理体现在可能重置节点或创建新分支，以保持树的平衡和表达力。多Agent场景无涉及。使用时还会根据记忆容量与重要性动态修剪低价值节点。

- **检索机制**: 检索时，系统会"压平"（Collapse）整个树状结构，将所有节点（无论层级深浅）放入一个扁平列表中进行相似度比较。这使得用户查询可以同时与高层级的抽象概念和底层的具体细节进行匹配。检索结果是Top-k个最相似的节点 。
- **检索模块的训练**: 免训练。
- **检索后处理**: 存在一个过滤步骤，会移除相似度低于预设阈值的节点 。

**13\. Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning (Memory-R1)**
检索依赖于**Answer-Agent**策略。模型接收问题和当前记忆内容，通过调优的Answer-Agent LLM自动筛选出最相关的记忆条目。检索模块（Answer-Agent）用强化学习训练，使其更好地定位有用条目。初筛后，模型直接整合这些条目生成答案，无多级后处理。**Memory-Manager**同样通过RL决定对记忆库进行增删改，可能在使用后更新条目。无多智能体分发。

- **检索机制**: 初始检索基于标准的RAG（Retrieval-Augmented Generation），即向量相似度检索 。
- **检索模块的训练**: 系统的核心是两个通过强化学习（PPO和GRPO）微调的Agent。其中，"记忆管理器"（Memory Manager）学习何时以及如何对记忆库执行结构化操作（ADD, UPDATE, DELETE, NOOP），这间接影响了可检索的内容。"回答Agent"（Answer Agent）则学习如何处理检索到的信息 。
- **检索后处理**: 一个非常关键的后处理步骤是"记忆蒸馏"（Memory Distillation）。在初始RAG检索出大量（例如60条）相关记忆后，"回答Agent"会对其进行筛选、推理和整合，最终"蒸馏"出一段简洁、高度相关的上下文，用于生成最终答案。这是一个主动的、智能的过滤和融合过程 。

**14\. Coarse-to-Fine Grounded Memory for LLM Agent Planning (CFGM)**
检索使用"关键环境信息"作为查询基础，选择相关的粗粒度经验和细粒度提示。主要依靠LLM将当前状态"归结"到最近的记忆焦点，然后检索对应经验。论文未提到专门训练检索模块，主要依赖提示和规则。检索结果包括粗略经验和详细提示，两者都作为上下文返回模型，支持计划。若遇异常，模型可细化查询并"自问自答"修正计划。无多Agent特性。后处理包括在推理中对检索到提示进行深入讨论（自反省）以动态修正计划。

- **检索机制**: 在推理阶段，系统会检索与当前任务相关的历史经验和"技巧"来支持规划 。
- **检索模块的训练**: 未提及。
- **检索后处理**: 未提及。
- **使用时的额外操作**: 当Agent在执行中遇到环境异常时，会触发一个"自问自答反思"（Self-QA Reflection）机制。Agent首先将当前情况提炼为细粒度的关键信息，然后基于这些信息以及过去的成功经验进行反思，以修正后续的计划 。

**15\. Multiple Memory Systems for Enhancing the Long-term Memory of Agent (MMS)**
检索通过匹配"检索单元"。系统用用户查询去匹配与之最相关的检索单元（例如通过文本相似度或向量匹配），然后找到对应的上下文单元。检索过程本质上是一对一：先检索最相关的检索单元，再取其配对的上下文单元作为答案上下文。没有训练检索器，匹配可通过关键词或向量简单实现。没有多智能体场景。使用时会控制所选记忆段数（单元数）对性能的影响，无额外复杂操作。

- **检索机制**: 检索分为两步。首先，系统使用用户的查询与"检索记忆单元"进行匹配，这些单元被设计为易于高效检索。一旦匹配到最相关的检索单元，系统就会利用它们之间的一对一链接，获取对应的、内容更丰富的"上下文记忆单元"作为最终的知识增强来源 。
- **检索模块的训练**: 未提及。
- **检索后处理**: 未提及。

**16\. Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory (TiM)**
在回答前，代理用LSH从记忆中**回忆**相关思考（向量相似度检索）；回答后，通过"后思考"生成的新思想以此形式存入记忆。检索器即LSH索引，通过训练过的哈希函数高效定位相关思考，无额外微调。检索后不再二次排名。多Agent未涉及。动态修正记忆：每次回答生成后将新思考写入记忆，旧无关思考可逐渐淘汰。

- **检索机制**: 为了应对大规模记忆库的检索效率问题，TiM采用了局部敏感哈希（LSH）。LSH能够将相似的项（这里是"想法"的嵌入）映射到相同的桶中，从而将全局搜索问题转化为局部搜索，显著提高了检索速度 。
- **检索模块的训练**: 免训练。
- **检索后处理**: 不适用。
- **使用时的额外操作**: TiM的核心操作发生在响应生成_之后_的"后思考"（Post-thinking）阶段。LLM会对最近的交互进行反思，生成新的归纳性想法，或者将新想法与已有想法进行合并，然后更新记忆库 。

**17\. A-MEM: Agentic Memory for LLM Agents (A-MEM)**
每增添一条记忆笔记后，系统自动**分析历史记忆**并为新旧笔记建立链接。检索时，模型通过查看新问题中的关键词和标签，直接访问与之关联的笔记节点。记忆系统本身无专门训练；检索依赖图结构中的邻接关系（网络中的连边）和记忆管理策略。检索后，相关笔记被顺序或主题关联地呈现给模型，无需再排序。多Agent场景未提及。额外操作包括：插入新记忆时对历史记忆的上下文和标签进行更新，实现记忆的连续演化。

- **检索机制**: 采用两阶段检索方法。第一阶段是"语义匹配"，利用向量嵌入计算查询与所有笔记的相似度，召回Top-k个最相关的笔记。第二阶段是"上下文扩展"（Contextual Expansion），系统会自动包含所有与第一阶段召回的笔记有显式链接的其他笔记。这使得返回的上下文是一个互联的知识簇，而不仅仅是孤立的事实 。
- **检索模块的训练**: 免训练。
- **检索后处理**: 未提及。
- **使用时的额外操作**: 记忆的演化是自动的。当新记忆被链入网络时，可能会触发对现有记忆的关键词、标签或上下文描述的修改，实现记忆网络的持续自我完善 。

**18\. Zep: A Temporal Knowledge Graph Architecture for Agent Memory (Zep)**
检索基于时序知识图查询。对当前对话内容进行语义解析后，系统查找图中时间上相近或语义相关的节点。Graphiti 能够融合多源数据，所以查询不仅是文本匹配，还考虑历史事实关系。检索结果综合输出关联事实，提供给LLM。系统不需针对检索训练。多智能体应用时通常代表整个对话历史，不分智能体。检索后不需要额外后处理，由知识图本身保证时序连贯。使用中，记忆层通过持续添加新对话节点维护时态关系。

- **检索机制**: 检索API接收自然语言查询，并通过一系列步骤将其转化为对时态知识图谱的查询。这可能涉及到实体链接、关系抽取和图遍历算法，以从三个子图中提取与查询相关的、具有时间有效性的信息 。
- **检索模块的训练**: 未提及。
- **检索后处理**: 未提及。

**19\. MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents (MEM1)**
每轮将新观察与旧记忆融合更新紧凑内部状态，无传统检索步骤。记忆系统通过RL训练得到，当代理需要回答时仅使用当前内部状态状态向量，无向量数据库检索。无需额外排序或多Agent分发。使用时重要操作是**记忆压缩与丢弃**：模型在更新内部状态时会有意识地过滤掉冗余信息，实现长期任务内存恒定增长。

- **检索机制**: 不存在独立的检索模块。记忆（内部状态）始终在上下文窗口中。
- **检索模块的训练**: 记忆的"使用"和"更新"过程是通过端到端的强化学习（PPO）进行训练的。Agent通过在交互中不断生成新的内部状态来整合新信息和压缩历史，其奖励完全基于任务的最终成功与否。强制的上下文修剪迫使Agent必须学会高效的记忆管理才能获得奖励 。
- **检索后处理**: 不适用。

**20\. Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science (Nemori)**
检索依赖分割出的事件记忆。系统使用"两步对齐"原则将对话切分为事件，然后通过"预测-校正"原则确定当前查询所属事件。检索即确定当前对话属于哪段事件上下文，并提取该事件及相邻预测误差相关内容。没有训练检索模块。检索后模型以这些事件内容辅助回答。无多Agent记忆分发。额外操作包括：自动学习新知识以解决预测误差（将新信息加入记忆）、定期重新划分记忆单元。

- **检索机制**: 未详细描述，但其双重记忆表征（情景记忆和语义记忆）暗示可能存在多模式的检索策略，以分别访问这两种不同类型的记忆。
- **检索模块的训练**: 未提及。
- **检索后处理**: 未提及。

**21\. Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory (Mem0)**
分为基础和平面图两种检索模式。基础版动态提取对话**关键事实**作为检索内容；图版使用对话实体与关系构建的知识图进行查询。检索无额外训练，由系统定向提问目前对话状态然后在内存中匹配相关事实。基础版结果直接拼接，无后处理；图版可利用图遍历获取多跳关联。无多智能体场景。使用时关键操作包括：新对话信息抽取为事实条目并加入图/记忆列表，旧事实根据时效性被逐渐屏蔽，以控制记忆规模。

- **检索机制**: Mem0的检索基于对提取出的简洁事实进行匹配。Mem0g的检索则是在图结构上进行的，能够利用实体间的关系进行更复杂的推理式检索 。
- **检索模块的训练**: 未提及。
- **检索后处理**: 未提及。
- **使用时的额外操作**: 系统包含一个异步的摘要生成模块，该模块会定期刷新对话的长期摘要，确保在不阻塞实时响应的前提下，记忆上下文保持最新 。

### 2.2 本章总结

对记忆使用与检索方法的分析揭示了该领域从静态信息提取向动态、智能决策的深刻转变。

首先，一个显著的趋势是**从固定的检索算法向可学习的记忆策略演进。** 早期的或较简单的方法（如CER）依赖于固定的、基于提示或相似度计算的检索函数。这种方式虽然有效，但缺乏适应性，因为记忆的"相关性"往往是高度依赖于任务和当前上下文的。为了克服这一局限，许多前沿工作（如Memento, MemAgent, Memory-R1, MEM1）开始将记忆管理本身视为一个序贯决策问题，并利用强化学习来训练一个"策略"。这个策略不仅决定"检索什么"，还决定"更新什么"、"遗忘什么"甚至"如何压缩记忆"。这标志着记忆模块本身正在变得"Agent化"，它不再是一个被动的外部设备，而是一个拥有自身学习能力和行为策略的智能组件。

其次，**检索后处理与信息综合的重要性日益凸显。** 研究者们普遍认识到，简单地将检索到的多个信息片段堆砌到LLM的上下文中是低效的，甚至可能因为引入噪声而干扰核心推理。因此，一个明确的、在检索之后进行的综合处理步骤成为先进系统设计的关键一环。例如，Memory-R1的"记忆蒸馏"和AGENT KB的"精炼"步骤 ，都是利用LLM自身强大的语言能力，对初步检索到的大量、原始的信息进行二次加工——总结、筛选、融合、适配——最终形成一段为当前任务量身定制的、简洁而高效的上下文。这体现了向多阶段推理流水线的演进，其中记忆检索只是第一步，其后必须跟进专门的综合与规划环节。

最后，**多Agent系统的特殊需求正在催生专门化的、多层次的检索机制。** 单Agent系统的记忆主要关乎其自身的历史经验。而多Agent系统（MAS）的记忆则需要捕捉整个团队的集体历史和复杂的交互动态。G-Memory的双向遍历策略正是为此而生 ：一个新任务可能既需要借鉴从过往十几个不相关任务中总结出的高层战略（从"洞察图"中检索），也需要精确回忆五分钟前Agent A与Agent B之间的具体对话内容（从"交互图"中检索）。单一的检索机制无法同时满足这两种截然不同的需求，因此必须设计能够访问不同粒度记忆的多层次检索策略，这是单Agent系统通常不具备的特性。

| **论文** | **核心检索与处理方法概述** |
| --- | --- |
| CER | 通过提示让 LLM 从经验缓冲区返回 top-k 相关经历；无需额外训练，结果直接拼接进上下文；无二次排序，持续更新缓冲区并合并重复内容。 |
| In-Mem Learning | 推理阶段将查询与笔记一起输入模型，依赖笔记支持；无检索模块或训练；笔记实时更新，无后处理机制。 |
| ASI | 以代码工具形式检索技能：代理可调用已诱导的程序技能库；无传统检索训练，技能调用由语言模型自身判断；检索后直接执行技能，无排序；新技能可新增入库。 |
| AWM | 通过匹配任务目标从工作流记忆中选取相关条目（可能是简单匹配或模型判断）；不需训练检索器；选中工作流直接用作提示；无再排序；新增成功的任务轨迹可归纳出新工作流并加入记忆。 |
| Memp | 构建查询向量或关键词检索记忆（试验了多种键策略）；无端到端训练；检索后可能合并或过滤条目；采用动态更新机制持续校正和废弃过时内容。 |
| AGENT KB | 采用双阶段图检索：学生代理从"工作流知识"中检索宏观策略，教师代理从"执行日志"中检索细节；无训练；检索结果由模型Refine融合；无多Agent分发，多框架共享知识库。 |
| MemAgent | 无显式检索：RL 控制记忆覆盖策略，将新对话与记忆状态结合；无需训练检索器；记忆更新视作动作策略；无排序；主要关注记忆管理策略。 |
| LearnToMem | 使用可学习的专家门控（MoE）将多种记忆相关性打分综合；检索部分通过对比学习/策略优化强化；结果通过可学习聚合层融合；后续去冗余、更新权重等无监督调整。 |
| Memento | **Answer-Agent** 模型对记忆进行检索，**Memory-Manager**通过RL学习写入更新；Answer-Agent 选择相关记忆条目，无额外排序；Memory-Manager 可能更新或删除记忆以优化效果。 |
| G-Memory | 在层级知识图上双向检索：同时查找高层洞见和细粒度交互轨迹；无训练；检索结果直接辅助推理；跨Agent共享记忆图；检索后将新轨迹并入图，演化记忆层。 |
| EM-LLM | 两阶段检索：先基于相似度检索相关事件，再基于时间邻近检索上下文；不训练模型；结果合并返回；无多Agent；事件分割机制动态管理记忆单元。 |
| MemTree | 通过语义嵌入动态调整树结构：新信息与已有节点比较，插入相应位置；无需训练；可能调整树平衡；无多Agent。 |
| Memory-R1 | **Memory Manager** 和 **Answer Agent** 均用PPO/RL训练：前者学习执行写入/更新/删除，后者学习选取相关记忆；Answer-Agent 选条目后生成答案；无多Agent。 |
| Coarse-to-Fine | LLM 将当前状态归纳为关键焦点，再检索关联的粗粒记忆和细提示；无专门训练；检索结果包括经验和提示，两者组合使用；若发现异常则进一步细化查询并校正计划。 |
| MultipleMem | 利用查询匹配"检索单元"，再提取对应的上下文单元；无需训练；一对一映射检索；仅返回对应上下文；没有广播；记忆单元数量参数可调。 |
| TiM | 使用 LSH 从向量化记忆中回忆思考；随后"后思考"新内容并写入记忆；LSH索引无需训练；检索后无重排序；无多Agent。 |
| A-MEM | 新笔记加入后自动分析历史，建立相似性链接；检索时通过图上邻接获取相关笔记；无训练；返回相关笔记链；无多Agent；新记忆还能触发旧记忆属性更新。 |
| Zep | 基于时序知识图查询相关节点；无训练；图查询即可；检索后无排序；跨会话全局图，无多Agent；新对话自动更新图。 |
| MEM1 | 内部状态整合新的观察以代替原始检索；使用RL训练该状态更新策略；无显式检索步骤；无多Agent；主要操作为记忆集中和丢弃冗余。 |
| Nemori | 系统自动将对话流分割为事件，自上而下确定当前查询相关事件；无需训练；直接使用该事件内容；无多Agent；事件分割和新知识学习动态演化记忆。 |
| Mem0 | 基础版提取对话中的重要事实直接检索；图版使用对话知识图进行查询；无训练；检索到相关事实后并入上下文；无多Agent；支持在线添加新事实并剪枝旧事实。 |

#### 按检索机制归纳的论文分类

| **检索机制** | **代表论文** | **核心特点** |
| --- | --- | --- |
| **提示式检索** | CER, AWM, Coarse-to-Fine | 使用LLM提示来选择最相关的记忆，无需额外训练，直接依赖模型判断 |
| **相似度检索(不同记忆见无图连接)** | Memp, Learn to Memorize, Memento, TiM, Memory-R1 | 基于向量相似度或关键词匹配检索相关记忆，支持高效查找 |
| **图结构检索** | AGENT KB, G-Memory, Zep, A-MEM, Mem0g | 通过图遍历和查询机制，支持复杂关系和多层次检索 |
| **树结构检索** | MemTree | 通过树遍历和相似度比较，支持层次化记忆检索 |
| **关键词匹配检索** | MMS | 基于关键词匹配检索单元，支持一对一映射 |
| **两阶段检索** | EM-LLM, AGENT KB, A-MEM | 结合相似度和时间邻近等多维度检索，提高准确性 |
| **RL策略检索** | MemAgent, Learn to Memorize, Memory-R1, MEM1 | 通过强化学习训练策略，动态决定检索内容 |
| **压缩或总结窗口内的上下文** | In-Memory Learning, EM-LLM, MEM1 | 无显式检索，直接利用上下文中的记忆内容 |

#### 按检索模块的训练归纳的论文分类

| **训练方法** | **代表论文** | **核心特点** |
| --- | --- | --- |
| **免训练** | CER, In-Memory Learning, ASI, AWM, Memp, AGENT KB, G-Memory, EM-LLM, MemTree, Coarse-to-Fine, MMS, TiM, A-MEM, Zep, Nemori, Mem0 | 不需要专门训练检索模块，依赖预设规则或模型固有能力 |
| **强化学习训练** | MemAgent, Learn to Memorize, Memento, Memory-R1, MEM1 | 通过RL算法训练检索策略，实现自适应记忆管理 |

#### 按检索后处理归纳的论文分类

| **后处理方法** | **代表论文** | **核心特点** |
| --- | --- | --- |
| **无后处理/直接拼接** | CER, In-Memory Learning, ASI, AWM, Memp, G-Memory, EM-LLM, MemTree, MMS, TiM, Zep, Nemori, Mem0 | 检索结果直接使用，无额外加工步骤 |
| **精炼/蒸馏** | AGENT KB, Memory-R1, Learn to Memorize, Coarse-to-Fine | 利用LLM对检索结果进行二次加工、筛选和整合 |
| **合并/融合** | Learn to Memorize, TiM, A-MEM | 将多个检索到的记忆项进行合并或融合处理 |
| **过滤** | MemTree | 根据相似度阈值过滤不相关的记忆项 |

#### 按使用时的额外操作归纳的论文分类

| **额外操作** | **代表论文** | **核心特点** |
| --- | --- | --- |
| **记忆更新** | CER, In-Memory Learning, ASI, AWM, Memp, Learn to Memorize, Memento, Memory-R1, TiM, A-MEM, Nemori, Mem0 | 持续更新记忆内容，包括修正、合并、废弃过时信息 |
| **多Agent分发** | AGENT KB, G-Memory, Zep | 在多Agent系统中分发记忆，支持协作任务 |
| **动态调整** | MemAgent, Learn to Memorize, Memento, Memory-R1, MEM1 | 根据上下文动态调整检索偏好或记忆策略 |
| **反思/后思考** | TiM, Coarse-to-Fine, In-Memory Learning | 利用反思机制生成新记忆或调整现有记忆 |

## 3\. 记忆的储存与利用 (Memory Storage and Utilization)

本章探讨记忆内容的本质形态，即Agent究竟储存了什么。这涉及到对原始交互经验的处理深度，是从最原始的日志，到经过层层提炼的抽象知识。分析这一层面，有助于理解不同记忆系统在保真度、通用性和推理能力之间的权衡。

### 3.1 论文分析

**1\. Contextual Experience Replay for Continual Learning of Language Agents (CER)**
储存内容为**蒸馏后的经历**（经验），不是原始交互日志。具体地，系统离线收集代理任务的轨迹后，通过引导LLM提炼关键经验（环境动态+技能组合）生成记忆条目，然后将其存入缓冲区。因此不保留原始日志用于检索，只存摘要经验。记忆利用时：提取出的经验直接加入系统提示，供后续任务参考，不对原始轨迹作二次查询。不存在多Agent情形。

- **储存内容的形态**: 储存的是经过高度处理和抽象后的信息。系统并不保存原始的交互轨迹，而是通过一个"蒸馏模块"将其转化为两种文本摘要："环境动态"和"技能" 。
- **加工处理与抽象**: 加工方法是利用LLM进行提示式蒸馏。LLM被指令去总结网页的关键信息（生成"环境动态"）或从一系列动作中归纳出可复用的操作流程（生成"技能"）。输出的产物是结构化的文本，其中具体实例被泛化为变量，以增强通用性 。这些抽象后的信息通过上文提到的VLM提示式方法进行检索。

**2\. In-memory learning A declarative learning framework for large language models (IML)**
保存的是**抽象的知识笔记**（经归纳的规则或启发式短文），而非原始问题-回答轨迹。每个"生物"有自己的笔记集合。原始数据（由任务定义的数据集）仅在**归纳阶段**用于生成笔记，不作为长期存储。检索时直接使用笔记文本，不需要针对原始数据查询。笔记加工通过LLM的"归纳-修正"迭代实现（类似梯度更新过程），加工后笔记即为记忆源，并通过推理阶段直接调用。

- **储存内容的形态**: 储存的是从经验中抽象出的"笔记"或通用原则，以纯文本形式存在于上下文窗口中 。
- **加工处理与抽象**: 加工方法是"归纳"（Induction）。Agent（即LLM）分析其近期的交互轨迹，并从中提炼出可以指导未来行为的、更具普适性的规则。这个过程完全通过自然语言在内存（上下文）中完成 。

**3\. Inducing Programmatic Skills for Agentic Tasks (ASI)**
记忆存储为**程序技能**（代码）和辅助说明文本。训练过程中，系统收集成功执行任务的轨迹（可视为原始日志），并在**诱导阶段**调用LLM生成可执行代码作为新技能。成功验证（通过执行检查）后，将该代码加入技能库。此后任务时，直接在上下文中提供相关代码。原始轨迹仅用于技能生成，不用于查询。多智能体不适用。加工为：轨迹序列→抽象总结（文本）→生成代码（技能）。

- **储存内容的形态**: 储存的是抽象后的程序化技能，即Python函数代码 。
- **加工处理与抽象**: 加工方法是"程序化技能归纳"。一个归纳模块（同样由LLM实现）接收一段成功的交互轨迹，并尝试将其中的一部分动作序列泛化成一个带有参数的、可执行的Python函数。这个过程的产物是一个封装了底层操作的高级技能 。

**4\. Agent Workflow Memory (AWM)**
存储内容为**可重复使用的工作流**。工作流程由正确解决任务的轨迹（OBS-Action对序列）归纳生成，每条轨迹都对应一个工作流条目（文本描述+步骤序列）。工作流从"正确解决任务"的执行结果中诱导而来。存储后，检索时按需把工作流加入提示。原始轨迹（点击步骤等）不单独查询，而是通过工作流来抽象。加工过程：成功轨迹 → LM提炼共性 → 形成新的工作流条目，然后该条目可被未来任务复用。

- **储存内容的形态**: 储存的是抽象出的"工作流"（Workflows），这是一种结构化的文本表示 。
- **加工处理与抽象**: 加工方法是"工作流归纳"。一个归纳模块（LLM）从成功轨迹中识别出常见的子任务序列，并将其抽象成一个工作流。抽象过程包括为工作流撰写高层目标描述，并记录下每一步的环境状态、推理逻辑和具体动作，同时将实例特定的信息（如"干猫粮"）替换为变量（如{product-name}）。

**5\. Memp: Exploring Agent Procedural Memory (Memp)**
存储内容为两种类型：**详细指令**和**脚本抽象**。原始轨迹（每步操作及环境状态）首先被蒸馏为可读指令和流程脚本，作为记忆条目。例如一段完整轨迹可能分解为简明的动作步骤和概括更高层次流程。记忆录入时会把这些文本写入库中。查询时通过关键词或向量检索得到相关条目。故记忆存储的是经过加工抽象的知识点，而不是完整的交互日志。

- **储存内容的形态**: 储存的是经过提炼的程序化知识，分为细粒度的指令和高层次的脚本式抽象 。
- **加工处理与抽象**: 加工方法是"蒸馏"。系统从过去的Agent轨迹中提取出操作知识。产物是两种不同抽象级别的文本：一种是详细的分步指南，另一种是更概括的、类似脚本的流程描述 。

**6\. AGENT KB: Leveraging Cross-Domain Experience for Agentic Problem Solving (AGENT KB)**
维护一个**分层记忆库**。原始交互日志被分为"工作流级策略"和"执行级日志"两部分，分别提取出高层策略和具体执行细节存入两个层级。也就是说，它对交互数据进行了**两级抽象**：先总结出高层流程，然后记录具体步骤。所有数据被组织到知识图中，做索引使用。原始轨迹不直接用于查询，只作为构建知识图的源数据。记忆使用时，学生代理使用工作流层的知识，教师代理使用细节层的知识指导推理。

- **储存内容的形态**: 储存的是高度抽象和结构化的经验元组。在多Agent系统中，维护的是一个全局共享的知识库 。
- **加工处理与抽象**: 加工方法是将执行轨迹抽象为形式化的元组 E=⟨π,γ,S,C,R⟩。这个过程将原始的交互日志转化为了包含问题模式、解决方案、特征和关系链接的结构化知识单元，便于跨领域迁移和复用 。

**7\. MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent (MemAgent)**
记录内容为对话历史的紧凑状态向量，而非明文轨迹。原始对话逐轮读入，Agent 根据学习的策略有选择地将重要内容保留到内部记忆中，其他丢弃。未保持完整交互日志，只将"抽象"后的关键信息保存在上下文窗口里。查询时直接利用这个内部状态。加工方式是**在线记忆合并**，将新信息与旧记忆合并或替换，效果类似不断更新的摘要。

- **储存内容的形态**: 储存的是对所有已处理信息的压缩表征，形态为Token序列，存在于上下文窗口中 。
- **加工处理与抽象**: 加工过程即记忆的更新过程，由RL训练的模型来执行。模型在读取每个新的文本块后，决定如何更新其固定长度的记忆Token序列，以最优地保留对未来任务有用的信息。这是一种隐式的、基于学习的压缩和抽象 。

**8\. Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework (Learn to Memorize)**
存储内容包括用户会话中的关键信息或任务要点（原文或摘要），作为记忆库条目。输入对话的消息经处理后（可能摘录主题句或答案）存储为记忆。原始对话未完整储存，而是提取后的要素。记忆利用时，将多轮对话的关键信息用作背景。加工方式包括对话主题识别和信息压缩，存入多维度记忆向量，且训练出聚合函数合并多条检索结果。

- **储存内容的形态**: 储存的是从观察中提取的关键信息，是经过处理的抽象内容 。
- **加工处理与抽象**: 加工方法是"任务相关的反思"（Task-specific Reflection）。系统通过一个可学习的、针对特定任务的指令 ptask 来引导LLM从新的观察 st 中提取关键信息 mt。这个指令 ptask 本身可以通过对成功和失败轨迹的反思来优化，从而调整记忆储存的"注意力" 。

**9\. Memento: Fine-tuning LLM Agents without Fine-tuning LLMs (Memento)**
记忆条目为**问答对或交互案例**。原始对话日志被拆分为问题-答案对，当作记忆条目存入。查询时根据新问题匹配和检索最相近的旧Q&A。新增经验时，系统可能更新或替换已有条目。故存储的是经过筛选的历史案例，而非完整的聊天记录。后续利用这些案例帮助回答类似问题；加工主要是通过网络训练（RL）优化存储策略，记忆条目本身无需进一步加工。

- **储存内容的形态**: 储存的是接近原始形态的交互轨迹，即完整的情景记忆，形式为 (s,a,r) 元组序列 。
- **原始轨迹的查询**: 查询这些轨迹是通过案例推理的方式，即寻找与当前状态 s 最相似的历史案例来进行的。

**10\. G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems (G-Memory)**
在多智能体环境中，记忆条目包括先前对话的摘要与跨任务洞见。原始合作轨迹被划分并压缩成"洞见"和"交互摘要"节点存入图中。也就是说，系统对长交互进行了聚合（把多个交互内容抽象为一个节点）。查询时在图中检索相关节点，不直接访问原始对话。记忆的加工方式是层级抽象：将多个对话片段归纳成更高层次的知识，再将其插入三层图中。

- **储存内容的形态**: 同时储存了多个抽象层次的信息。在最底层是细粒度的交互轨迹（消息），在其之上是与用户查询关联的轨迹集合，最高层则是从多个查询中总结出的通用"洞察"。在多Agent系统中，这是一个全局共享的系统交互记忆 。
- **加工处理与抽象**: 加工过程是分层进行的。首先，原始的交互被记录。然后，这些交互被组织在特定的查询之下。最后，系统会跨多个查询进行分析，以归纳出更高层次的、可泛化的"洞察"。

**11\. Human-inspired Episodic Memory for Infinite Context LLMs (EM-LLM)**
记忆条目为**事件块**，是对连续文本按照认知事件划分的片段。原始文本在线划分为事件，每个事件的文本（或者其摘要）存为记忆单元。并不保存没有经过切分的长文本，检索时直接定位事件单元。加工通过"贝叶斯惊奇"确定事件边界，将对话切分为更有意义的片段，然后存入。

- **储存内容的形态**: 储存的是模型内部的KV缓存块，代表了对输入文本流的分割，可以看作是半处理过的信息 。
- **加工处理与抽象**: 加工方法是"事件分割"。系统利用贝叶斯惊奇（Bayesian Surprise）和图论优化，在线地将连续的Token流（及其对应的KV对）切分成语义连贯的"事件"块。这个过程的产物就是一系列被分割好的KV缓存块 。

**12\. From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs (MemTree)**
记忆存储为树节点，每个节点包含聚合过的文本信息。原始对话逐步插入到树中：新信息与节点内容比较，相似则合并或提升抽象层级，不相似则创建新节点。最终每个节点代表汇总后的对话摘要，取代了冗长的日志。检索时对话先定位到相关节点即可得到上下文。该方法的加工过程是动态递归：计算新信息与现有节点嵌入相似度，并相应地更新节点内容或结构。

- **储存内容的形态**: 储存的是在树状结构中按层次组织的聚合文本。每个节点的内容都是对其所有子节点内容的抽象和概括 。
- **加工处理与抽象**: 加工方法是"层级聚合"。当一个新信息被添加到树的某个叶子节点时，从该叶子节点到根节点路径上的所有父节点都会被更新。更新操作由一个LLM执行，它会读取一个节点当前的内容和其所有子节点的内容，然后生成一个新的、更抽象的概括性文本作为该节点的新内容 。

**13\. Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning (Memory-R1)**
记忆条目为**问答对**或辅助信息对。论文训练了一个Memory-Manager来决定对记忆条目（问题、答案、相关上下文）进行增、删、改操作。原始对话被转换为若干问答对存入记忆库，后续查询时按问题检索相关答案对。记忆经常更新和重写，以纠正错误或添加新信息。故存储经过滤、标注过的问题答案，而非流水式聊天。加工在训练阶段完成：通过RL优化确定何时写入或替换记忆内容。

- **储存内容的形态**: 储存的是以文本形式存在的、原子化的事实（Facts）。
- **加工处理与抽象**: 加工由RL训练的"记忆管理器"Agent来执行。它会分析新的交互信息，并决定是向记忆库中ADD一个新的事实，UPDATE一个已有的事实，还是DELETE一个过时或错误的事实。这个过程将非结构化的对话历史转化为了结构化的、离散的事实陈述 。

**14\. Coarse-to-Fine Grounded Memory for LLM Agent Planning (CFGM)**
存储**经验与提示**两层信息。训练阶段先使用粗粒度焦点总结关键经验（作为记忆条目），再为每条经验生成多层次行动要点。原始轨迹被分解为"经验焦点"和"行动提示"文本后存储。查询时检索到的经验与提示直接用于规划。加工过程包括：在训练中用LLM把环境信息归纳为焦点，再从每个焦点的经验中提取细节提示作为记忆条目。

- **储存内容的形态**: 同时储存了原始轨迹和抽象信息。其"经验池"中包含完整的、带有成功或失败标签的交互轨迹。其"技巧字典"中则储存了从这些轨迹中抽象出的可操作建议 。
- **原始轨迹的查询**: 轨迹是按任务进行索引的。
- **加工处理与抽象**: 加工方法是"经验化技巧提取"。一个LLMTips模块会对比同一个任务的成功和失败轨迹，从中总结出避免错误的技巧。它也会分析成功的轨迹，提炼出导致成功的关键因素。产物是文本形式的、具有指导意义的"技巧" 。

**15\. Multiple Memory Systems for Enhancing the Long-term Memory of Agent (MMS)**
储存为成对的单元："检索单元"与"上下文单元"。原始对话被分割为若干片段，每片段产生一个检索单元（简化关键词）和对应的上下文单元（完整内容）。查询时检索单元被匹配，上下文单元则用于回答。加工是：将历史对话切成多个段并抽取关键检索信息存单元对。并不保存整段长对话，仅保留关键摘要和其上下文对。

- **储存内容的形态**: 储存的是经过多层次处理的"记忆片段" 。
- **加工处理与抽象**: 加工方法模仿了认知心理学中的"处理层次理论"。系统通过一个多层次的分析过程，从短期记忆中提取出关键词、多维认知视角、情景记忆和语义记忆等多种不同类型的记忆片段。这些片段随后被组装成"检索单元"和"上下文单元" 。

**16\. Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory (TiM)**
记忆为"历史思考"。原始对话在代理生成回答后，立刻进入"后思考"，生成一个高层次总结或洞见（思想）并存入记忆。这样，记忆内容是代理对话的自省结果，而不是原始消息。使用时回答前先检索（回忆）这些思想。加工包括：每轮额外执行后思考步骤，用于更新记忆，以避免模型重复推理。

- **储存内容的形态**: 储存的是抽象的"想法"（Thoughts），而非原始对话历史 。
- **加工处理与抽象**: 加工过程是"后思考"。在每次交互结束后，LLM会反思刚刚发生的对话，并生成归纳性的结论或提炼关键信息，形成新的"想法"。系统还支持对已有想法进行"合并"（Merge）操作，将关于同一主题的多个零散想法整合成一个更全面的想法 。

**17\. A-MEM: Agentic Memory for LLM Agents (A-MEM)**
每当有新经验需要记忆时，系统生成**带结构属性的笔记**（包含上下文描述、关键词、标签等）。原始对话被加工为摘要笔记及其元数据存储。记忆利用时模型可通过关键词或链接访问相关笔记。记忆加工流程：记录新经验→生成结构化笔记并插入→扫描历史笔记建立关联和更新属性。

- **储存内容的形态**: 储存的是结构化的"综合笔记"，是高度加工后的信息 。
- **加工处理与抽象**: 加工方法是"笔记生成"。对于每一段新的交互，一个LLM会被调用来为其生成一张包含多个属性的笔记，包括上下文描述、关键词和标签。这个过程将原始的、非结构化的交互转化为一个富含元数据的、半结构化的知识单元 。

**18\. Zep: A Temporal Knowledge Graph Architecture for Agent Memory (Zep)**
记忆条目为对话事实或业务数据的融合项。原始对话和外部数据实时合并到**知识图**中：每段对话生成对应节点并加入图中，原始消息被转换为图数据库中的事实。查询则直接对图进行检索。加工是：将对话内容解析为图实体和关系，写入图里；不会保存未经处理的文本。

- **储存内容的形态**: 同时储存了原始数据和抽象知识。其"情景子图"储存了原始的交互数据（称为Episodes），而"语义实体子图"则储存了从中提取出的实体和关系 。
- **加工处理与抽象**: 加工方法是知识图谱构建流程。系统从原始的Episodes中进行命名实体识别和关系抽取，生成实体节点和关系边，并为它们打上时态标签，最终构建出语义实体子图 。

**19\. MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents (MEM1)**
记忆以内部状态向量形式存在，原始对话通过代理与内部状态交互。代理接收新观察后通过RL更新内部状态（合并或丢弃旧信息）。因此并不存储显式的对话日志，而是不断压缩与更新为一组向量表示。查询时只使用该内部状态。加工过程是：对话信息被及时吸收或遗忘，使内部状态既包含历时上下文也支持当前推理。

- **储存内容的形态**: 储存的是对历史交互的动态、压缩的文本摘要，即"内部状态" 。
- **加工处理与抽象**: 加工过程是"记忆整合"（Memory Consolidation）。在每个交互轮次，Agent（由RL训练的LLM）会读取旧的内部状态和新的观察信息，然后生成一个全新的、更紧凑的内部状态来取代它们。这个过程会战略性地丢弃不相关或冗余的信息，是一种持续的、在线的抽象过程 。

**20\. Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science (Nemori)**
按**事件片段**存储对话。系统使用"两步对齐"原则自动将对话切分为语义连贯的**事件**。每个事件的摘要（或关键句）作为记忆条目，被存入记忆系统。查询时检索与当前对话属于同一或相关事件的记忆。原始对话仅用于确定事件边界，不被直接检索。记忆加工是：在线执行事件分段和预测误差学习（将差异转化为新知识），由此自我更新记忆内容。

- **储存内容的形态**: 储存的是分割后的文本"情景"，以及从中蒸馏出的"语义记忆" 。
- **加工处理与抽象**: 加工过程分为两步。第一步是"边界对齐"，即利用一个智能边界检测器将对话流分割成情景。第二步是"预测-校准"，系统会主动地从情景中提炼和升华出更稳定的语义知识 。

**21\. Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory (Mem0)**
记忆条目为会话中的**关键事实**。基础版从对话中动态提取事实并存储为文本；增强版则将对话中的实体和关系构建成图谱。原始对话不会原样保存，只保留提炼出的信息。查询时直接匹配事实（或查询知识图）。加工过程是：每次对话后自动提取重要信息作为新的记忆条目，并可在图中更新节点/边；定期清除或压缩过时信息。


- **储存内容的形态**: 储存的是从对话中提取的简洁、原子化的事实。在Mem0g版本中，还储存了这些事实背后的实体和关系图 。
- **加工处理与抽象**: 加工过程是一个"提取-更新"流水线。一个提取模块（LLM）会从最近的对话、滚动摘要等多个信息源中提取出一组候选的记忆事实。然后，这些事实会被用来更新记忆库 。

### 3.2 本章总结

对记忆内容形态的分析揭示了该领域的一个核心共识和两个关键的设计选择。

核心共识是：**抽象是实现有效记忆的关键，而非对历史的完美复刻。** 在所分析的21篇论文中，绝大多数都选择了储存经过处理和抽象后的信息，而非原始的交互日志。其背后的逻辑是，原始数据充满了噪音、冗余和特定于某一情境的细节，直接储存和复用价值有限。通过将原始经验抽象为通用的"技能"（CER, ASI）、"工作流"（AWM）、"想法"（TiM）或"原则"（IML），Agent才能实现知识的泛化，并将从一次经验中学到的教训应用到未来的、相似但不同的情境中。这表明，研究界普遍认为，一个高效记忆系统的核心能力不在于"记住过去"，而在于"从过去中学习并泛化"。

在此共识之上，存在一个重要的设计分支：**LLM不仅是记忆的使用者，更是其自身的策展人（Curator）。** 几乎所有进行抽象处理的系统，都依赖LLM本身来执行这一关键任务。无论是CER的"蒸馏"、AWM的"归纳"、TiM的"后思考"，还是A-MEM的"笔记生成"，其核心都是通过精心设计的提示来引导一个LLM对原始信息进行总结、提炼和泛化。这形成了一个强大而有趣的自举循环：Agent未来的表现依赖于其记忆的质量，而记忆的质量又取决于其背后LLM的抽象和推理能力。这也意味着，基础模型在归纳、总结和泛化能力上的任何提升，都可能直接转化为其作为Agent时记忆性能的提升，即便记忆系统的架构本身保持不变。

另一个关键的设计选择体现在**程序化知识（"知道如何做"）与陈述性知识（"知道是什么"）的权衡上。** 不同的记忆系统根据其Agent的目标任务，选择了不同形态的抽象知识。像ASI, AWM, Memp这类专注于在交互环境中执行多步任务（如网页导航）的系统，倾向于将经验抽象为程序化的知识，如可执行的技能、工作流或脚本。这种形态的记忆直接指导Agent的行动。而像TiM, A-MEM, IML这类更侧重于问答、对话和知识管理任务的系统，则倾向于将经验抽象为陈述性的知识，如事实、想法、原则或关联的笔记。这种形态的记忆主要用于增强Agent的理解和推理能力。这一分野清晰地表明，记忆内容的最佳形态与Agent的最终应用场景紧密耦合。

下表总结了每篇论文在记忆储存内容与利用方式上的核心方法。

| **论文** | **储存内容及利用概览** |
| --- | --- |
| CER | 存储经提炼的**经历**（环境动态+技能对）文本，不保存原始日志；使用时直接将相关经验文本填入提示；无多Agent。 |
| In-Mem Learning | 存储归纳出的**笔记**（生物属性规则），不存对话；推理时将笔记带入上下文，无原始轨迹查询；迭代更新笔记。 |
| ASI | 存储**程序技能代码**和说明文本；在诱导阶段通过对轨迹归纳生成代码；使用时模型调用代码，无查询原始日志；技能可跨任务共享。 |
| AWM | 存储可复用的**工作流**（文本+步骤序列），由成功轨迹归纳生成；检索时直接使用相关工作流条目辅助规划；不保存完整轨迹。 |
| Memp | 存储两层程序性记忆：**操作指令**和**流程脚本**，由轨迹蒸馏生成；检索时根据关键词/向量匹配条目；更新机制包括新增、过滤、废弃。 |
| AGENT KB | 存储跨领域经验的**策略与日志**：总结出高层次工作流和详细执行日志；利用时通过知识图查找相应层面信息；不存原始对话。 |
| MemAgent | 存储固定维度的内部状态向量（记忆+当前信息）；原始对话更新为向量状态；查询时直接使用状态，无原始日志。 |
| LearnToMem | 存储用户对话中的**关键信息**（如概念、要点）；对话内容被摘要后录入记忆；检索时根据多维策略匹配相关信息；更新包括合并和过滤。 |
| Memento | 存储问题-答案对或案例；在推理阶段检索相似案例；Memory-Manager可更新条目；不保留完整对话，只保留案例集。 |
| G-Memory | 存储多轮协作的摘要和见解；原始轨迹抽象成图节点；检索时在记忆图中查找相关事件；多Agent共享一套记忆图，无个性分发。 |
| EM-LLM | 存储**事件块**（对话分段）；原始长对话按事件切分后存储；检索时直接取用相关事件内容；无多Agent。 |
| MemTree | 存储汇总对话内容的树节点；新输入与节点合并或生成节点；查询时定位相关节点即可；不保存原文。 |
| Memory-R1 | 存储问答对或辅助信息对；Memory-Manager 可能修改条目；检索时根据问题选答案对；不存历史对话。 |
| Coarse-to-Fine | 存储两类经验：粗粒度焦点（关键信息）和细粒度提示；对话通过模型抽取关键信息和提示并存为记忆；检索直接获得相关经验和提示；不保留原对话。 |
| Multiple (MMS) | 存储成对单元：一个关键词摘要（检索单元）与对应详细上下文；对话被分段处理生成单元；检索时先匹配关键词，再取其上下文；不保留对话全貌。 |
| TiM | 存储"思考"结果；每轮对话后模型进行"后思考"并保存；检索时回忆相关历史思想；原始消息不存内存中。 |
| A-MEM | 存储结构化**笔记**（含描述、关键词、标签）；每条新经验生成为笔记并插入，模型可检索；维护笔记网络；不保留原始文本。 |
| Zep | 存储融合对话和业务数据的**知识图**；对话事实作为节点写入图；检索时查询图；不单独存对话文本，使用图结构保持多会话一致性。 |
| MEM1 | 存储为内部状态向量；对话信息被即时吸纳或丢弃；查询时只使用状态；不保存对话内容。 |
| Nemori | 存储对话**事件**摘要；对话自动分段，每段生成事件记忆；查询时选取同事件记忆；不保留完整对话；记忆随新的预言错误动态学习更新。 |
| Mem0 | 存储提取的**关键信息**或构建的知识图；基础版存文本事实，增强版存图结构；不保存会话原文；新信息加入记忆、旧信息被淘汰。 |

# 4\. 评估使用的 Benchmark

以下列出了每篇论文实验中使用的基准测试（Benchmark）名称：

- **Contextual Experience Replay (CER)**：WebArena、VisualWebArena。
- **In-Memory Learning**：自定义 10 维"生物属性分类"任务集。
- **Inducing Programmatic Skills (ASI)**：WebArena。
- **Agent Workflow Memory (AWM)**：WebArena、Mind2Web。
- **Memp**：TravelPlanner、ALFWorld。
- **AGENT KB**：GAIA，SWE-bench、DeepResearcher。
- **MemAgent**：HotpotQA、RULER。
- **Learn to Memorize**：HotpotQA、MemDaily。
- **Memento**：GAIA、SWE-bench、DeepResearcher。
- **G-Memory**：多个多Agent任务集。
- **EM-LLM**：LongBench、∞-Bench。
- **MemTree**：MSC、MSC-E、QuALITY、MultiHop RAG。
- **Memory-R1**：LoCoMo(子集)。
- **Coarse-to-Fine**：AlfWorld, WebShop, ScienceWorld。
- **Multiple Memory Systems (MMS)**：LoCoMo。
- **Think-in-Memory (TiM)**：GVD、Kdconv、RMD。
- **A-MEM**：LoCoMo。
- **Zep**：MemGPT 提出的 Deep Memory Retrieval (DMR) 基准，以及作者提出的 LongMemEval 基准。
- **MEM1**：HotpotQA、Natural Question、WebShop。
- **Nemori**：LoCoMo 和 LongMemEval 。
- **Mem0**：LoCoMo 。

# 5\. 按时间线的综合分析与统计

## 5.1 论文发表时间分布表

| 年份  | Q1  | Q2  | Q3  | Q4  |
| --- | --- | --- | --- | --- |
| 2023 | 0   | 0   | 0   | 1 (Think-in-Memory) |
| 2024 | 1 (In-memory learning) | 0   | 1 (AWM) | 1 (MemTree) |
| 2025 | 4 (A-MEM, Zep, CER, EM-LLM) | 4 (G-Memory, Mem0, ASI, MEM1) | 9 (Memp, AGENT KB, MemAgent, Learn to Memorize, Memento, Memory-R1, Coarse-to-Fine, MMS, Nemori) | 0   |

## 5.2 Benchmark随时间使用分布表

| Benchmark | 2023-Q4 | 2024-Q1 | 2024-Q3 | 2024-Q4 | 2025-Q1 | 2025-Q2 | 2025-Q3 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| WebArena | — | — | ✓ (AWM) | — | ✓ (CER) | ✓ (ASI) | — |
| VisualWebArena | — | — | — | — | ✓ (CER) | — | — |
| Mind2Web | — | — | ✓ (AWM) | — | — | — | — |
| TravelPlanner | — | — | — | — | — | — | ✓ (Memp) |
| ALFWorld | — | — | — | — | — | — | ✓ (Memp, Coarse-to-Fine) |
| WebShop | — | — | — | — | — | ✓ (MEM1) | ✓ (Coarse-to-Fine) |
| ScienceWorld | — | — | — | — | — | — | ✓ (Coarse-to-Fine) |
| GAIA | — | — | — | — | — | — | ✓ (AGENT KB, Memento) |
| SWE-bench | — | — | — | — | — | — | ✓ (AGENT KB, Memento) |
| DeepResearcher | — | — | — | — | — | — | ✓ (AGENT KB, Memento) |
| LongBench | — | — | — | — | ✓ (EM-LLM) | — | — |
| ∞-Bench | — | — | — | — | ✓ (EM-LLM) | — | — |
| HotpotQA | — | — | — | — | — | ✓ (MEM1) | ✓ (MemAgent, Learn to Memorize) |
| Natural Question | — | — | — | — | — | ✓ (MEM1) | — |
| RULER | — | — | — | — | — | — | ✓ (MemAgent) |
| MemDaily | — | — | — | — | — | — | ✓ (Learn to Memorize) |
| DMR | — | — | — | — | ✓ (Zep) | — | — |
| LongMemEval | — | — | — | — | ✓ (Zep) | — | ✓ (Nemori) |
| LoCoMo | — | — | — | — | ✓ (A-MEM) | ✓ (Mem0) | ✓ (Memory-R1, MMS, Nemori) |
| Custom (分类) | — | ✓ (In-Memory Learning) | — | — | — | — | — |
| GVD/Kdconv/RMD | ✓ (TiM) | — | — | — | — | — | — |
| MSC/MSC-E/QuALITY/MultiHop RAG | — | — | — | ✓ (MemTree) | — | — | — |
| 多个多Agent任务集 | — | — | — | — | — | ✓ (G-Memory) | — |

# 论文原文

1.  **Contextual Experience Replay for Continual Learning of Language Agents**: [https://openreview.net/forum?id=RXvFK5dnpz](https://openreview.net/forum?id=RXvFK5dnpz)
2.  **In-memory learning A declarative learning framework for large language models**: [https://arxiv.org/abs/2403.02757](https://arxiv.org/abs/2403.02757)
3.  **Inducing Programmatic Skills for Agentic Tasks**: [https://arxiv.org/abs/2504.06821](https://arxiv.org/abs/2504.06821)
4.  **Agent Workflow Memory**: [https://arxiv.org/abs/2409.07429](https://arxiv.org/abs/2409.07429)
5.  **Memp: Exploring Agent Procedural Memory**: [https://arxiv.org/abs/2508.06433](https://arxiv.org/abs/2508.06433)
6.  **AGENT KB: Leveraging Cross-Domain Experience for Agentic Problem Solving**: [https://arxiv.org/abs/2507.06229](https://arxiv.org/abs/2507.06229)
7.  **MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent**: [https://arxiv.org/abs/2507.02259](https://arxiv.org/abs/2507.02259)
8.  **Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework**: [https://arxiv.org/abs/2508.16629](https://arxiv.org/abs/2508.16629)
9.  **Memento: Fine-tuning LLM Agents without Fine-tuning LLMs**: [https://arxiv.org/abs/2508.16153](https://arxiv.org/abs/2508.16153)
10. **G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems**: [https://arxiv.org/abs/2506.07398](https://arxiv.org/abs/2506.07398)
11. **Human-inspired Episodic Memory for Infinite Context LLMs**: [https://openreview.net/forum?id=BI2int5SAC](https://openreview.net/forum?id=BI2int5SAC)
12. **From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs**: [https://arxiv.org/abs/2410.14052](https://arxiv.org/abs/2410.14052)
13. **Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning**: [https://arxiv.org/abs/2508.19828](https://arxiv.org/abs/2508.19828)
14. **Coarse-to-Fine Grounded Memory for LLM Agent Planning**: [https://arxiv.org/abs/2508.15305](https://arxiv.org/abs/2508.15305)
15. **Multiple Memory Systems for Enhancing the Long-term Memory of Agent**: [https://arxiv.org/abs/2508.15294](https://arxiv.org/abs/2508.15294)
16. **Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory**: [https://arxiv.org/abs/2311.08719](https://arxiv.org/abs/2311.08719)
17. **A-MEM: Agentic Memory for LLM Agents**: [https://arxiv.org/abs/2502.12110](https://arxiv.org/abs/2502.12110)
18. **Zep: A Temporal Knowledge Graph Architecture for Agent Memory**: [https://arxiv.org/abs/2501.13956](https://arxiv.org/abs/2501.13956)
19. **MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents**: [https://arxiv.org/abs/2506.15841](https://arxiv.org/abs/2506.15841)
20. **Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science**: [https://arxiv.org/abs/2508.03341](https://arxiv.org/abs/2508.03341)
21. **Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory**: [https://arxiv.org/abs/2504.19413](https://arxiv.org/abs/2504.19413)